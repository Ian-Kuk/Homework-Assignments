{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU317i1xSudP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guidelines for data plotting\n",
        "\n",
        "When asked to produce a plot, you must:\n",
        "\n",
        "1. Follow directions on whether to use a line plot or scatter plot\n",
        "2. Provide descriptive labels for all axes and give units when indicated\n",
        "3. Provide a descriptive title for the plot\n",
        "4. Provide a descriptive legend for the plot when plotting more than one dataset\n",
        "5. Use color or marker to differentiate between datasets when plotting more than one dataset\n",
        "\n",
        "### Guidelines for report numerical and text-based answers\n",
        "\n",
        "1. When asked to report a numerical value (e.g. a validation score), you must include a print statement that describes the value being reported either together or directly before the printing of the value itself.\n",
        "\n",
        "2. When asked to answer a question with text, create a new text cell and answer the question in complete sentances."
      ],
      "metadata": {
        "id": "dbTxjdPESy-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Estimating the Accuracy of Linear Regression Model\n",
        "\n",
        "The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a machine learning model. Here we use the bootstrap approach to assess the variability of the estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, the intercept and slope terms for the linear regression model. We will build a linear regression model that predicts `mpg` (miles per gallon) as a function of `horsepower` using the `Auto` dataset."
      ],
      "metadata": {
        "id": "XVQUDzeaTQpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load dataset\n",
        "\n",
        "**Load the dataset into a dataframe. Drop the `name` column. Remove all examples with missing values and ensure that the remaining columns all have numeric (`float`, `int`) datatypes.** You should not have any columns with `object` datatype."
      ],
      "metadata": {
        "id": "_pmWlxq4dw3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.statlearning.com/s/Auto.csv"
      ],
      "metadata": {
        "id": "5tXfRKBqTP-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oy8drg_kji_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Perform univariate linear regression\n",
        "\n",
        "Fit the `mpg` (y, dependent variable) as a function of `horsepower` (x, independent variable) using a univariate linear regression model\n",
        "\n",
        "$$\n",
        "y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
        "$$\n",
        "\n",
        "**Print out the value of the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$).**\n",
        "\n",
        "**Construct a plot showing the original dataset and model predictions.** The plot should contain (1) a scatter plot showing the `horsepower` (x-axis) and `mpg` (y-axis) and (2) a line plot showing the linear regression model. Be sure to follow all plotting guidelines."
      ],
      "metadata": {
        "id": "Px2YnoXTeyIA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIOb0NBbje0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Estimate the standard error of the linear coefficients using the Bootstrap method\n",
        "\n",
        "Produce $R=1000$ bootstrap estimates of the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$). Your solution should:\n",
        "\n",
        "1. Construct 1000 unique bootstrap datasets (sample with replacement) the same size as the original dataset (n=392).\n",
        "2. Fit a univariate regression model for each bootstrap dataset. Construct a collection object to store the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$) of each model for later use.\n",
        "3. Estimate the standard error of the mean for the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$) using the 1000 estimates. HINT: `scipy.stats.sem`\n",
        "\n",
        "**Print out the bootstrap esimate of the standard error of the slope and intercept.**"
      ],
      "metadata": {
        "id": "5DNCyqSGj7ic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLVVG4Jxj_dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Compute the standard error of the linear coefficients using formulas\n",
        "\n",
        "The standard error of the mean value of the linear coefficients can be estimated using standard formulas\n",
        "\n",
        "$$\n",
        "\\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\bigg[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\bigg]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n",
        "$$\n",
        "\n",
        "Here, $\\sigma$ is residual standard error that can be computed using the estimator\n",
        "\n",
        "$$\n",
        "\\sigma^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-2}\n",
        "$$\n",
        "\n",
        "Where $y_i$ is the true `mpg` of the $i$-th example and $\\hat{y}_i$ is the linear regression estimate. The $n-2$ in the denominator corresponds to the number of degrees of freedom in univariate linear regression. $n$ is the number of data points.\n",
        "\n",
        "**Print out the formula esimate of the standard error of the slope and intercept.**"
      ],
      "metadata": {
        "id": "EV4ysd0VkD9k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQQbuM4SmF-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE\n",
        "\n",
        "The standard error can be used to estimate the 95\\% confidence interval of the linear regression coefficients. There is a 95\\% chance that the interval $[\\hat{\\beta}_1 - 2 \\text{SE}(\\hat{\\beta}), \\hat{\\beta}_1 + 2 \\text{SE}(\\hat{\\beta})]$ will contain the true value of $\\hat{\\beta}_1$. And similar for $\\hat{\\beta}_0$.\n",
        "\n",
        "You should observe some difference between the two computed confidence intervals. The standard formulas used in Step 4 rely on the correct estimation of the variance $\\sigma^2$ from the residual error. These residual are inflated, and therefore so is $\\sigma^2$, due to the observed nonlinearity of the data. In this case, the bootstrap estimate is generally more correct."
      ],
      "metadata": {
        "id": "Qty5JZ7OqpCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Revisiting the Diabetic Retinopathy dataset with Discriminant Analysis\n",
        "\n",
        "This dataset contains features extracted from the Messidor image set to predict whether an image contains signs of diabetic retinopathy, an eye condition that can cause vision loss and blindness in people who have diabetes. All features (columns `0` - `18`) represent either a detected lesion, a descriptive feature of a anatomical part or an image-level descriptor. The final column (`Class`) is the class label where 1 = contains signs of diabetic retinopathy and 0 = no sign of diabetic retinopathy."
      ],
      "metadata": {
        "id": "74YySP4Px5OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Load the dataset"
      ],
      "metadata": {
        "id": "WFz0EoLV2HSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00329/messidor_features.arff"
      ],
      "metadata": {
        "id": "1avCWA41sZ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.arff import loadarff\n",
        "raw_data = loadarff('messidor_features.arff')\n",
        "df = pd.DataFrame(raw_data[0]).astype({'Class': int})\n",
        "df"
      ],
      "metadata": {
        "id": "sU21LWmu2Og7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Prepare the data for modeling\n",
        "\n",
        "1. **Prepare the target data `y` containing the data found in the `Class` column.**\n",
        "2. **Prepare the design matrix `X` containing the data found in the remaining columns.** Be sure to drop the `Class` column before generating `X`.\n",
        "3. **Split the dataset into training and testing sets.**"
      ],
      "metadata": {
        "id": "rmvFyHR62Qm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Class'].values\n",
        "X = df.drop('Class', axis=1).values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234)"
      ],
      "metadata": {
        "id": "9lXZo_9U2Qwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Model Selection\n",
        "\n",
        "1. **Construct a modeling `sklearn.pipeline.Pipeline` for fitting the data.** The pipeline should include:\n",
        "  - `sklearn.preprocessing.StandardScaler` standardization transformation.\n",
        "  - `sklearn.naive_bayes.GaussianNB` Gaussian Naive Bayes model.\n",
        "2. **Perform 5-fold cross-validation to evaluate the cross-validation score.**\n",
        "3. **Repeat the analysis, replacing the `GaussianNB` model** with\n",
        "  - `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` LDA model.\n",
        "  - `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` QDA model.\n",
        "4. **Print out the best cross-validation score and corresponding model.**"
      ],
      "metadata": {
        "id": "EKjlAdqO2Q7r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTuEYsdK2RFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Model testing and confusion matrix\n",
        "\n",
        "Using the best model from step 2, retrain the model on the training set. **Generate a confusion matrix based on ground truth data and predictions of the model on the testing set. Identify the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) model predictions.**"
      ],
      "metadata": {
        "id": "q42KMN2W2RNY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbNNJoop2RWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Model precision\n",
        "\n",
        "**Compute the `precision_score` for the positive and negative classes based on the ground truth data and predictions of the model on the testing set. Comment on the how well we are able to trust positive classifications and negative classifications given by our model.**"
      ],
      "metadata": {
        "id": "1Ygva_aA3dFh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5M_xlH83dMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Revisiting the Steel Plate Fault Analysis dataset with Random Forest\n",
        "\n",
        "The goal of this learning task is to correctly classify the type of surface defects in stainless steel plates, with seven types of possible defects. The features are a set of 27 indicators that approximately describe the geometric shape of the defect and its outline.\n",
        "\n",
        "The feature attribute information can be found in Homework 03."
      ],
      "metadata": {
        "id": "ylceLipEyciA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Load the dataset"
      ],
      "metadata": {
        "id": "PSng07Lmycn0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F98WBatw4RZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Prepare the data for modeling\n",
        "\n",
        "For this homework problem, **we will focus on the `Bumps` prediction task.** **Split the dataset into features (`X`) and task/labels (`y`). Split the features/labels into training and testing sets.** Be sure to include only the `Bumps` task in the labels array, and be sure to exclude all task/label data from the features."
      ],
      "metadata": {
        "id": "XqHL4IhH4RrE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-r_K9Pt4R0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Hyperparameter optimization\n",
        "\n",
        "1. **Construct a modeling `sklearn.pipeline.Pipeline` for fitting the data.** The pipeline should include:\n",
        "  - `sklearn.preprocessing.StandardScaler` standardization transformation.\n",
        "  - `sklearn.ensemble.RandomForestClassifier` random forest classification model.\n",
        "2. **Perform a grid search `GridSearchCV`** over\n",
        "  - `n_estimators`: the number of ensembled decision trees.\n",
        "  - `max_depth`: the maximum depth of a single decision tree.\n",
        "3. **Print out the best cross-validation score and corresponding parameters.**\n",
        "\n",
        "You may select the range of possible hyperparameter values, but you must compare at least 25 total models."
      ],
      "metadata": {
        "id": "qnuWV9kb4R_b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5AMIy3t4SMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Model testing and confusion matrix\n",
        "\n",
        "Using the best model from step 2, retrain the model on the training set. **Generate a confusion matrix based on ground truth data and predictions of the model on the testing set. Identify the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) model predictions.**"
      ],
      "metadata": {
        "id": "eutWpiLD4ScC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxLHamX44Spq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Model precision\n",
        "\n",
        "**Compute the `precision_score` for the positive and negative classes based on the ground truth data and predictions of the model on the testing set. Comment on the how well we are able to trust positive classifications and negative classifications given by our model.**"
      ],
      "metadata": {
        "id": "_hFbFBff4TSL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onFI1aM74TZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}